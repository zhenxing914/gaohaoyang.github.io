## 1. 数据集类型

1. 有界数据集

   有界数据集具有时间边界，在处理过程中数据一定会在某个时间范围内起始和结束，有可能是一分钟，也有可能是一天内的交易数据。对有界数据集的数据处理方式被称为批计算。

2. 无界数据集

   对于无界数据集，数据从开始生成就一直持续不断的产生新的数据，因此数据是没有边界的，例如服务器的日志、传感器信号数据等。对无界数据集的数据处理方式被称为流式数据处理过程实现复杂度会更高。

    

3. 统一数据处理

   目前在业界比较熟知的开源大数据处理框架中，能够同时支持流式计算和批量计算， 比较典型的代表分别为Apache Spark和Apache Flink两套框架。

   其中Spark通过批处理模式 来统一处理不同类型的数据集，对于流数据是将数据按照批次切分成微批(有界数据集) 来进行处理。

   Flink从另一个角度出发，通过流处理模型来统一处理不同类型的数据集。

## 2. Flink编程接口

Flink根据数据集类型的不同将核心数据处理接口分为两大类，一类是支持批计算的接口DataSet API，另外一类是支持流计算的接口DataStream API。

Flink将数据处理接口抽象成四层。

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdyuhbuoorj30o60bsadi.jpg" alt="image-20200419100042474" style="zoom:50%;" />



## 3. Flink程序结构

```scala
package com.realtime.flink.streaming
import org.apache.flink.api.java.utils.ParameterTool
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecution Environment, _}
object WordCount {
  def main(args: Array[String]) {
  // 第一步:设定执行环境设定
    val env = StreamExecutionEnvironment.getExecutionEnvironment // 第二步:指定数据源地址，读取输入数据
    val text = env.readTextFile("file:///path/file")
    // 第三步:对数据集指定转换操作逻辑
    val counts: DataStream[(String, Int)] = text
    .flatMap(_.toLowerCase.split(" ")) .filter(_.nonEmpty)
    .map((_, 1))
    .keyBy(0)
    .sum(1)
    // 第四步:指定计算结果输出位置 if (params.has("output")) {
    counts.writeAsText(params.get("output")) } else {
    println("Printing result to stdout. Use --output to specify output path.")
    counts.print() }
    // 第五步:指定名称并触发流式任务
    env.execute("Streaming WordCount") }
}

```



## 4. Flink数据类型

### 1. 数据类型支持

1. 原生数据类型

   Flink通过实现BasicTypeInfo数据类型，能够支持任意java原生基本类型（装箱）或String类型，例如Integer、String、Double等，如下代码所示，通过从给定的元素集中创建DataStream数据集。

   ```scala
   //创建Int类型的数据集
   val intStream:DataStream[Int] = env.fromElements(3, 1, 2, 1, 5) //创建String类型的数据集
   val dataStream: DataStream[String] = env.fromElements("hello", "flink")
   ```

   

2. Java Tuple类型

    ```scala
   //通过实例化Tuple2创建具有两个元素的数据集
   val tupleStream2: DataStream[Tuple2[String, Int]] = env.fromElements(new Tuple2("a",1), new Tuple2("c", 2))
   ```

   

3. Scala Case Class类型

    ```scala
   //定义WordCount Case Class数据结构
   case class WordCount(word: String, count: Int) //通过fromElements方法创建数据集
   val input = env.fromElements(WordCount("hello", 1), WordCount("world", 2)) val keyStream1 = input.keyBy("word") // 根据word字段为分区字段，
   val keyStream2 = input.keyBy(0) //也可以通过指定position分区
   ```

   

   

4. POJOs类型

    ```scala
   //定义Java Person类，具有public修饰符 
   public class Person {
   //字段具有public修饰符 public String name; public int age;
   //具有默认空构造器
   public Person() {
   }
   public Person(String name, int age) {
   this.name = name;
   this.age = age; }
   }
   ```

   ```scala
   val persionStream = env.fromElements(new Person("Peter",14),new Person("Linda",25))
   //通过Person.name来指定Keyby字段
   persionStream.keyBy("name")
   Scala POJOs数据结构定义如下，使用方式与Java POJOs相同。 class Person(var name: String, var age: Int) {
                 
   } }
   //默认空构造器 def this() {
   this(null, -1)
     
   ```

   

   

5. Flink Value类型

    Value数据类型实现了org.apache.flink.types.Value，其中包括read()和write()两个方法完成序列化和反序列化操作，相对于通用的序列化工具会有着比较高效的性能。目前Flink 提供了內建的Value类型有IntValue、DoubleValue以及StringValue等，用户可以结合原生 数据类型和Value类型使用。

   

6. 特殊数据类型

   在Flink中也支持一些比较特殊的数据数据类型，例如Scala中的List、Map、Either、 Option、Try数据类型，以及Java中Either数据类型，还有Hadoop的Writable数据类型。如 下代码所示，创建Map和List类型数据集。这种数据类型使用场景不是特别广泛，主要原 因是数据中的操作相对不像POJOs类那样方便和透明，用户无法根据字段位置或者名称获 取字段信息，同时要借助Types Hint帮助Flink推断数据类型信息。

```scala
//创建Map类型数据集
val mapStream = env.fromElements(Map("name"->"Peter","age"->18),Map("name"->"Linda", "age"->25))
//创建List类型数据集
val listStream = env.fromElements(List(1,2,3,5),List(2,4,3,2))
```

​    

### 2. TypeInformation信息获取

1. Scala API类型信息

    Scala API通过使用Manifest和类标签，在编译器运行时获取类型信息，即使是在函数 定义中使用了泛型，也不会像Java API出现类型擦除的问题，这使得Scala API具有非常精 密的类型管理机制。同时在Flink中使用到Scala Macros框架，在编译代码的过程中推断函 数输入参数和返回值的类型信息，同时在Flink中注册成TypeInformation以支持上层计算算 子使用。
    
    当使用Scala API开发Flink应用，如果使用到Flink已经通过TypeInformation定义的数据类型，TypeInformation类不会自动创建，而是使用隐式参数的方式引入，代码不会直接 抛出编码异常，但是当启动Flink应用程序时就会报“could not find implicit value for evidence parameter of type TypeInformation”的错误。这时需要将TypeInformation类隐式参 数引入到当前程序环境中，代码实例如下:

```scala
import org.apache.flink.api.scala._
```

   

2. JAVA API类型信息

   但是如果函数的输出类型不依赖于输入参数的类型信息，这个时候就 需要借助于类型提示(Ctype Hints)来告诉系统函数中传入的参数类型信息和输出参数信息。

```scala
DataStream<Integer> typeStream = input
	.flatMap(new MyMapFunction<String, Integer>())
	.returns(new TypeHint<Integer>() {	//通过returns方法指定返回参数类型
}); 
//定义泛型函数，输入参数类型为<T,O>,输出参数类型为O
class MyMapFunction<T, O> implements MapFunction<T, O> {
   public void flatMap(T value, Collector<O> out) {
//定义计算逻辑 
   }
}
```

​		在使用Java API定义POJOs类型数据时，PojoTypeInformation为POJOs类中的所有字段 创建序列化器，对于标准的类型，例如Integer、String、Long等类型是通过Flink自带的序 列化器进行数据序列化，对于其他类型数据都是直接调用Kryo序列化工具来进行序列化。

​		通常情况下，如果Kryo序列化工具无法对POJOs类序列化时，可以使用Avro对POJOs 类进行序列化，如下代码通过在ExecutionConfig中调用enableForceAvro()来开启Avro序列化。

```scala
ExecutionEnvironment env = 
ExecutionEnvironment.getExecutionEnvironment(); 
//开启Avro序列化方式 
env.getConfig().enableForceAvro();
```



3. 自定义TypeInformation

   