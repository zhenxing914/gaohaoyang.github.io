---
layout: post
title:  "5.决策树"
categories: "MachineLearning"
tags: "MachineLearning"
author: "songzhx"
date:   2019-11-18 14:53:00
---

​		决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。

## 1.决策树模型与学习

### 1.1 决策树模型

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jt8aqf9j30xa092ae5.jpg" alt="image-20191118150139558" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtbj75lj30xa0e8mzu.jpg" alt="image-20191118150154526" style="zoom:50%;" />



### 1.2 决策树与if-then规则





### 1.3 决策树与条件概率分布

​		决策树还表示给定特征条件下类的条件概率分布。



1.4 决策树学习

​		决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割。



 2.特征选择

 2.1 特征选择问题

​		同常特征选择的准则是信息增益或信息增益比。





2.2 信息增益

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtexz78j30u00y2wr9.jpg" alt="image-20191118170826906" style="zoom:50%;" />
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtil6s5j30y00h2dmf.jpg" alt="image-20191118170908206" style="zoom:50%;" />



信息增益的计算方法：

<img src="/Users/song/Library/Application Support/typora-user-images/image-20191118171058102.png" alt="image-20191118171058102" style="zoom:50%;" />
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtqjj22j30y00imjyg.jpg" alt="image-20191118171119097" style="zoom:50%;" />







2.3 信息增益比

​		信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比可以对这一问题进行校正。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtus1uij30y005oq4x.jpg" alt="image-20191118171615657" style="zoom:50%;" />



3.决策树的生成

3.1 ID3算法

<img src="/Users/song/Library/Application Support/typora-user-images/image-20191118172125498.png" alt="image-20191118172125498" style="zoom:50%;" />
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94jtyfbn4j30y00ls47g.jpg" alt="image-20191118172146895" style="zoom:50%;" />



3.2 C4.5的生成算法

​	<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94ju1v3qjj30y00kswmq.jpg" alt="image-20191118172232426" style="zoom:50%;" />



4.决策树的剪枝

​       决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的书往往对训练数据的分类很准确，但对未知的测试数据的分类确没有那么准确，即出现过拟合现象。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

​       在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94ju60of0j30y00d2gpl.jpg" alt="image-20191118175000336" style="zoom:50%;" />



5.CART算法

​       CART(classification and regression tree)模型是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，即可以用于分类也可以用于回归。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94ju9s9sqj30y00c2wjy.jpg" alt="image-20191118175331333" style="zoom:50%;" />





5.1 CART生成

**1.回归树的生成**



<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94judjic1j30y00ligud.jpg" alt="image-20191118175616508" style="zoom:50%;" />
<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g94juyif71j30y0034mxs.jpg" alt="image-20191118175645237" style="zoom:50%;" />



**2.分类树的生成**









5.2 CART剪枝





