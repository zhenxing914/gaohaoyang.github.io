## 1. Server安装

1. 源码安装

> 首先在https://www.apache.org/dyn/closer.cgi/atlas/2.0.0/apache-atlas-2.0.0-sources.tar.gz下载2.0.0源码

解压源码

1. 选择Atlas架构

Atlas支持多种架构作为后端

- HBase + Solr
- Cassandra + Solr

你可以选择多种，这里我们采用集成`HBase + Solr`方式编译

```shell
mvn clean -DskipTests package -Pdist,embedded-hbase-solr
```

执行代码即可（推荐使用阿里云的maven源加速编译）



1. 修改环境变量

编译完之后在`/distro/target`下面有很多tar.gz包，我们需要的是`apache-atlas-2.0.0-server.tar.gz`包，解压到当前目录

### 3.1 修改配置文件`conf/atlas-env.sh`

```properties
export JAVA_HOME=/your/java/install

export MANAGE_LOCAL_HBASE=false

export MANAGE_LOCAL_SOLR=false
```

我们设定`Solr`和`HBase`手动开启，方便我们发现哪个部分启动异常

### 3.2 修改admin密码：

系统默认会生成一个密码给我们，但是官网我也没看到说这个密码，所以我们自己生成一个，然后修改上去

```bash
echo -n "password" | sha256sum
```

使用上面命令生成一个`sha256`加密字符（你可以把password改成你想要的密码），复制生成的字符串（不需要`-`），例如`5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8`
修改`conf/users-credentials.properties` 改成

```bash
admin=ADMIN::5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8
```

### 3.3 修改HBase配置(需要提前安装好java和Zookeeper和Hadoop）

进入`hbase`目录夹

修改 `conf/hbase-env.sh`

```bash
export JAVA_HOME=/your/java/install
export HBASE_MANAGES_ZK=false
```



复制Hadoop配置到HBase中

```bash
cp $HADOOP_HOME/etc/hadoop/core-site.xml $HBASE_HOME/conf/
cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HBASE_HOME/conf/
```



在`hbase-site.xml`中加入

```properties
    <property>
         <name>hbase.cluster.distributed</name>
         <value>true</value>
    </property>
     <property>
        <name>hbase.rootdir</name>
        <value>/hbase</value> 
    </property>
    <property>
   			<name>hbase.zookeeper.quorum</name>
				<value>localhost</value>
    </property>
    <property>
    <name>hbase.zookeeper.property.clientPort</name>
				<value>2181</value>
    </property>
```

启动安装好的Zookeeper，使用`./bin/start-hbase.sh`启动`HBase`

使用`jps`应该能看到`HMaster`和`HRegionServer`启动了

测试HBase安装是否完成，使用`./bin/hbase shell` 进入`HBase` 命令行，如果`status`命令返回正确的话，那么你的HBase就安装好了



### 3.4 启动Solr

进入`solr`目录，启动`solr`

```bash
./bin/solr -c -z localhost:2181 -p 8983
```

打开`http://localhost:8983/solr/#/`如果能看到正常页面，那么Solr就启动好了

在`apache-atlas-2.0.0`目录下创建索引

```bash
./solr/bin/solr create -c vertex_index -d conf/solr -shards 1 -replicationFactor 1
./solr/bin/solr create -c edge_index -d conf/solr -shards 1 -replicationFactor 1
./solr/bin/solr create -c fulltext_index -d conf/solr -shards 1 -replicationFactor 1
```



### 3.5 启动Atlas

在`apache-atlas-2.0.0`目录下启动Atlas

使用`bin/atlas_start.py` 或者 `/usr/bin/python2.7 bin/atlas_start.py`

PS：第一次启动比较慢，如果前面的HBase和Solr都安装好了的话，一般都没有什么大问题，可以查看`logs/application.log`查看系统运行情况，等到初始化完成后打开`localhost:21000`使用`admin:password`即可登录上去

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge3qlu7gotj31pt0u0qby.jpg" alt="image-20200416163551716" style="zoom:50%;" />

当然我们现在系统空空如也，现在我们来使用Hook导入数据到Atlas里面去吧
我测试通过的版本是：Hadoop2.8.1 + Zookeeper 3.4.10 ，其他的都是用的默认Atlas 集成的版本





## 2. Hook安装

`Atlas`最强大的的地方就是能够把Hive，Sqoop，Kafka这些大数据组件的血缘关系给自动抽取出来，所以钩子的安装至关重要

### 1. Sqoop钩子

> Sqoop 我用的是`1.4.7`版本

- 配置Sqoop钩子

首先在`conf/sqoop-site.xml`中添加

```properties
<property>
  <name>sqoop.job.data.publish.class</name>
  <value>org.apache.atlas.sqoop.hook.SqoopHook</value>
</property>
```

- 复制必要的包

解压`distro/target`的`apache-atlas-2.0.0-sqoop-hook.tar.gz`，复制`apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook/sqoop/`目录到 `/hook/sqoop`

如：

```shell
cd /where/your/untar/atlas
cp -r ../../apache-atlas-2.0.0-sqoop-hook/apache-atlas-sqoop-hook-2.0.0/hook .
```

创建软链接`/atlas-application.properties`到`/`

如：

```shell
ln -s ln -s /home/zhanglun/github/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-server/apache-atlas-2.0.0/conf/atlas-application.properties /opt/sqoop-1.4.7.bin__hadoop-2.6.0/conf
```

将`/hook/sqoop/*.jar` 复制到`sqoop` `lib`目录

如：

```shell
cp hook/sqoop/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib
cp hook/sqoop/atlas-sqoop-plugin-impl/*.jar /opt/sqoop-1.4.7.bin__hadoop-2.6.0/lib
```

- 测试Sqoop 导入Hive中

sqoop import –connect jdbc:mysql://localhost:3306/sqoop
–username root
-P
–split-by id
–table root
–hive-import
–create-hive-table
–hive-table db.auth_user

不出意外应该会报错

```shell
Caused by: java.lang.NullPointerException
at org.apache.atlas.hook.AtlasHook
```

因为我们还没有配置好Sqoop钩子，接下来我们来配置Sqoop钩子

- 配置Atlas

前面我们创建了软链接，现在我们只要修改`conf/atlas-application.properties`这个配置即可

首先我们得配置关闭`Kafka`作为发送消息缓冲，因为Atlas默认使用`Kafka`作为消息缓冲，然后我们修改下面的配置（这个后期可以打开，你再配置好kafka的地址）

```shell
atlas.notification.embedded=false  # 不往kafka里面发送
atlas.graph.index.search.backend=solr5 
```

- 异常一： Caused by: java.lang.ClassNotFoundException: org.json.JSONObject

  包缺失，下载[java-json.jar](http://www.java2s.com/Code/Jar/j/Downloadjavajsonjar.htm) 到`Sqoop`文件夹

  - 异常二：`Import failed: java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly`

环境变量没有设置对，设置`HIVE_CONF_DIR`在`conf/sqoop-env.sh`（确保`HADOOP_HOME`和`HIVE_HOME`不是空值）

```
export HADOOP_CLASSPATH="`$HADOOP_HOME/bin/hadoop classpath`:$HIVE_HOME/lib/*"
```

- 异常三： `Error when removing metric from org.apache.kafka.common.metrics.JmxReporter java.security.AccessControlException: access denied ("javax.management.MBeanTrustPermission" "register")`

根据[stackoverflow](https://stackoverflow.com/questions/12195868/java-security-accesscontrolexception-when-using-ant-but-runs-ok-when-invoking-j) 解决

- 异常四：`java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor`

Hive包版本和Sqoop包版本冲突（我的Hive版本是2.3.4），可以先备份`Sqoop`的lib，文件再进行下面操作：

```
cp -r lib lib_back
rm lib/jackson-*
cp $HIVE_HOME/lib/jackson-* lib/
```

- 异常五：`Connection to node -1 could not be established`

你在`conf/atlas-application.properties`没有修改`atlas.notification.embedded`成false，那么你必须配置好`kafka`地址

```
atlas.kafka.zookeeper.connect=localhost:2181
atlas.kafka.bootstrap.servers=localhost:9092
```

PS：每次出现异常，你必须先删掉Hadoop上面的文件，再执行导入，你可以直接安装我的流程进行修复，因为这些都是我在配置的时候顺序出现的问题，走到这里我们就配置好了`Sqoop`和`Hive`的导入Hook，如果运行成功，你会看到下面界面。



### 2. Hive 钩子

接下来我们配置Hive钩子来导入Hive中的表。

#### 1. 配置hive-site.xml

在里面加入

```properties
<property>
    <name>hive.exec.post.hooks</name>
      <value>org.apache.atlas.hive.hook.HiveHook</value>
  </property>
```



#### 2. 解压 hive-hook包

如：`tar xzvf apache-atlas-2.0.0-hive-hook.tar.gz`

复制到atlas中

如：

```shell
cp -r apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0/hook/hive /usr/local/apache-atlas-2.0.0/hook/

cp -r apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0/hook-bin /usr/local/apache-atlas-2.0.0
```



#### 3. 配置Hive环境变量

在`hive-env.sh`中加入`'export HIVE_AUX_JARS_PATH=$ATLAS_PATH/hook/hive`



#### 4. 给创建软链接

像前面一样创建一个`atlas-application.properties` 和 atlas-env.sh 软链接到`hive/conf`目录下
如：

```shell
cd /usr/local/hive

ln -s /usr/local/apache-atlas-2.0.0/conf/atlas-application.properties  /usr/local/hive/conf/atlas-application.proper

ln -s /usr/local/apache-atlas-2.0.0/conf/atlas-application.properties  /usr/local/hive/conf/atlas-application.properties

```



#### 5. 复制Hive包到Hook

```shell
import-hive.sh

依赖Hive的jackson一些包（报java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.util.BeanUtil.okNameForGetter错误），

## 解决方法
把Hive的依赖包复制到钩子的包目录下面
如：cp $HIVE_HOME/lib/jackson-*  ../hook/hive/atlas-hive-plugin-impl/


## keberos访问问题
切换为hdfs用户sudo su - hdfs，cd /usr/local/apache-atlas-2.0.0/hook-bin 执行import-hive.sh

```

执行klist可以查看当前用户获取了票据。

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge3qm5q5mfj31gu07m0um.jpg" alt="image-20200423152546638" style="zoom:50%;" />



现在我们尝试执行`hook-bin/import-hive.sh`(在`apache-atlas-2.0.0-hive-hook/apache-atlas-hive-hook-2.0.0`目录下）



现在Atlas里面有两张表，不过一张是Sqoop导入的，一张是Hive导入的，查看Hive导入的血缘关系时候我们发现，他只有自己的一张表（源表）

其他Kafka和Storm的钩子比较简单我就不介绍详细过程了。

#### 6. kafka端创建topic

```bash
./bin/kafka-topics.sh --create --topic ATLAS_ENTITIES  --replication-factor 3 --partitions 3 --zookeeper 10.30.66.3:2181/kafka_es_2.0.1

./bin/kafka-topics.sh --create --topic ATLAS_HOOK --replication-factor 3 --partitions 3 --zookeeper 10.30.66.3:2181/kafka_es_2.0.1
```





## 3. 创建hive表

```sql
-- 指定表的仓库路径
CREATE EXTERNAL TABLE atlas_test(id STRING, name STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LOCATION '/atlastest';


-- 导入数据到表中(文件会被移动到仓库目录/data/test/test_table)
LOAD DATA LOCAL INPATH '/home/hdfs/atlas_test.txt' INTO TABLE atlas_test;


CREATE EXTERNAL TABLE atlas_t01(id STRING, name STRING) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ','
LOCATION '/atlastest';

vim /home/hdfs/atlas_test.txt 
1,zhangsan
2,lisi 


hive> select * from atlas_test ;
OK
1       zhangsan
2       lisi


drop table atlas_test；

```





## 4. 常见报错

### 1. hive缺少atlas配置

```shell
hive> show tables;
FAILED: Hive Internal Error: java.lang.ExceptionInInitializerError(null)
java.lang.ExceptionInInitializerError
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.atlas.hive.hook.HiveHook.initialize(HiveHook.java:72)
	at org.apache.atlas.hive.hook.HiveHook.<init>(HiveHook.java:41)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.lang.Class.newInstance(Class.java:442)
	at org.apache.hadoop.hive.ql.hooks.HookUtils.getHooks(HookUtils.java:61)
	at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1685)
	at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1669)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1941)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:239)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:153)
Caused by: java.lang.NullPointerException
	at org.apache.atlas.hook.AtlasHook.<clinit>(AtlasHook.java:81)
	... 28 more

hive>
```

解决办法： 

hive缺少了相应的脚本atlas-application.properties、atlas-env.sh，需要做一个软连接

```shell
ln -s /opt/dev/idea/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-bin/apache-atlas-2.0.0/conf/atlas-application.properties /opt/local/apache-hive-2.3.5-bin/conf/atlas-application.properties
ln -s /opt/dev/idea/apache-atlas-sources-2.0.0/distro/target/apache-atlas-2.0.0-bin/apache-atlas-2.0.0/conf/atlas-env.sh /opt/local/apache-hive-2.3.5-bin/conf/atlas-env.sh
```



### 2. hive hook问题

![image-20200423151624821](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge3qmbh6maj309i0j24a0.jpg)

解决方案：

在`hive-env.sh`中加入`'export HIVE_AUX_JARS_PATH=/usr/local/apache-atlas-2.0.0/hook/hive`

### 3. solr问题

```
2020-04-21 16:21:16,918 ERROR - [main:] ~ Failed to apply patches in file /usr/local/apache-atlas-2.0.0/models/0000-Area0/patches/001-base_model_replication_attributes.json. Ignored (AtlasTypeDefStoreInitializer:483)
org.apache.atlas.repository.graphdb.AtlasSchemaViolationException: org.janusgraph.core.SchemaViolationException: Adding this property for key [__patch.id] and value [TYPEDEF_PATCH_0000_001] violates a uniqueness constraint [__patch.id]
        at org.apache.atlas.repository.graphdb.janus.AtlasJanusElement.setProperty(AtlasJanusElement.java:121)
        at org.apache.atlas.repository.store.graph.v2.AtlasGraphUtilsV2.setProperty(AtlasGraphUtilsV2.java:239)
        at org.apache.atlas.repository.store.graph.v2.AtlasGraphUtilsV2.setEncodedProperty(AtlasGraphUtilsV2.java:207)
        at org.apache.atlas.repository.patches.AtlasPatchRegistry.createOrUpdatePatchVertex(AtlasPatchRegistry.java:134)
        at org.apache.atlas.repository.patches.AtlasPatchRegistry.register(AtlasPatchRegistry.java:95)
        at org.apache.atlas.repository.store.bootstrap.AtlasTypeDefStoreInitializer.applyTypePatches(AtlasTypeDefStoreInitializer.java:476)
        at org.apache.atlas.repository.store.bootstrap.AtlasTypeDefStoreInitializer.loadModelsInFolder(AtlasTypeDefStoreInitializer.java:209)
        at org.apache.atlas.repository.store.bootstrap.AtlasTypeDefStoreInitializer.loadBootstrapTypeDefs(AtlasTypeDefStoreInitializer.java:152)
        at org.apache.atlas.repository.store.bootstrap.AtlasTypeDefStoreInitializer.startInternal(AtlasTypeDefStoreInitializer.java:356)
        at org.apache.atlas.repository.store.bootstrap.AtlasTypeDefStoreInitializer.init(AtlasTypeDefStoreInitializer.java:114)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleElement.invoke(InitDestroyAnnotationBeanPostProcessor.java:366)
        at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor$LifecycleMetadata.invokeInitMethods(InitDestroyAnnotationBeanPostProcessor.java:311)
        at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:134)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:409)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1626)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483)
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:312)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:308)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
```

solr创建索引

```shell
./solr/bin/solr create -c vertex_index -d conf/solr -shards 1 -replicationFactor 1
./solr/bin/solr create -c edge_index -d conf/solr -shards 1 -replicationFactor 1
./solr/bin/solr create -c fulltext_index -d conf/solr -shards 1 -replicationFactor 1
```

### 4. luban端缺少包

解决办法

将hook中包导入至

![image-20200428102747601](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge99uqzzb8j30tg03yjul.jpg)

