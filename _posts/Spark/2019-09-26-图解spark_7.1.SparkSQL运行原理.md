---

layout: post
title:  "图解spark_7.SparkSQL运行原理"
categories: "Spark"
tags: "Spark "
author: "songzhx"
date:   2019-10-10 16:22:00 
---

## 1.SparkSQL简介

### 1.发展史

### 2. DataFrame、DataSet介绍

​	在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。

带来优点

1.精简代码

2.提升执行效率

3.减少数据读取



## 2. Spark SQL运行原理

### 1. 通用sql执行原理

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t74lm0uaj30s20bu78o.jpg" alt="image-20191010164253241" style="zoom:50%;" />



具体执行过程：

1.词法和语法解析

2.绑定

3.优化

4.执行

​	一般来说，关系数据库在运行过程中，会在缓冲池缓存解析过的SQL语句，在后续的过程中如果能够命中缓存SQL就可以直接返回可执行的计划。





### 2. SparkSQL运行框架

SparkSQL会先将SQL语句进行解析行程一个Tree，然后使用Rule对Tree进行绑定、优化等处理过程，通过模式匹配对不同类型的节点采用不同的操作。



**1.Tree**

在SparkSQL中会根据语法生成一棵树，该树一直在内存中维护，不会保存在磁盘。

Tree有两个子类继承体系，即QueryPlan和Expression



**2.Rule**

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7uf8dmgvxj30te0e6q80.jpg" alt="image-20191011180846240" style="zoom:50%;" />



SparkSQL主要组件的功能以及SQL语句执行的运行流程

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7ufag3juhj30xa0bsdlu.jpg" alt="image-20191011181043337" style="zoom:50%;" />

1. 将SQL语句通过此法和语法解析生成未绑定的逻辑计划，然后在后续步骤中使用不同的Rule应用到该逻辑计划上。

2. Analyzer使用Analysis Rules，配合数据元数据，完善未绑定的逻辑计划的属性而转换成绑定的逻辑计划。

3. Optimizer使用Optimization Rules，将绑定的逻辑计划进行合并、列裁剪和过滤器下推等优化工作后生成优化的逻辑计划。

4. Planner使用Planning Stategies，对优化的逻辑计划进行转换生成可执行的物理计划。根据过去的性能统计数据，选择最佳的物理执行计划，最后可以执行的物理执行计划树，即得到SparkPlan。

5. 最最终真正执行计划前，还要进行preparations规则处理，最后调用SparkPlan的execute执行计算RDD。

   

### 2.3.SQLContext运行原理

1.使用SessionCatalog保存元数据



2.使用Antlr生成未绑定的逻辑计划



3.使用Analyzer绑定逻辑计划



4.使用Optimizer优化逻辑计划



5.使用SparkPlanner生成可执行的物理计划



6.使用QueryExecution执行物理计划



### 2.4.HiveContext介绍



## 3. 使用Hive-Console

### 3.1.编译Hive-Console



### 3.2.查看执行计划



### 3.3.应用Hive-Console





## 4.使用SQLConsole

1.启动HDFS和Spark Shell

2.与RDD交互操作

3.读取JSON格式数据

4.读取Parquet格式数据

5.缓存演示

6.DSL演示



## 5.使用Spark SQL CLI

1.配置并启动Spark SQL CLI

2.实战Spark SQL CLI



## 6.使用Thrift Server

1.配置并启动Thrift Server

2.基本操作

3.交易数据实例

4.使用IDEA开发实例




