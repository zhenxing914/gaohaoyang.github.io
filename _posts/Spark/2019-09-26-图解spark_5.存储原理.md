---

layout: post
title:  "图解spark_5.存储原理"
categories: "Spark"
tags: "Spark "
author: "songzhx"
date:   2019-09-26 14:22:00 
---

## 1.存储分析

### 1.1 整体架构

​	Spark的存储采取了主从模式，整个存储模块使用前面介绍的RPC的消息通信方式。其中，Master负责整个应用程序运行期间的数据块元数据的管理和维护。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7cwlbizkbj30y40mg14j.jpg" alt="image-20190926142920184" style="zoom:60%;" />

- （1）在应用程序启动时，SparkContext会创建Driver端的SparkEnv，在该SparkEnv中实例化BlockManager和BlockManagerMaster。

  ​		在Executor启动时也会创建SparkEnv，在该SparkEnv中实例化BlockManager和负责网络数据传输服务的BlockTransferService。在BlockManager初始化过程中，一方面会加入BlockManagerMasterEndpoint终端点，
  
  

- （2）当写入、更新或删除数据完毕后，发送数据块的最新状态消息UpdateBlockInfo给BlockManagerMasterEndpoint终端点，由其更新数据块的元数据。

  

- （3）应用程序数据存储后，在获取远程节点数据、获取RDD执行的首选位置等操作时需要根据数据块的编号查询块所处的位置。

  

- （4）Spark提供删除RDD、数据块和广播变量等方式。

下面以RDD的unpersistRDD方法描述其删除过程。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7d0avc3iwj30y40gggna.jpg" alt="image-20190926145339348" style="zoom:60%;" />

​		1.在sparkContext中调用unperistRDD方法，在该方法中发送removeRdd消息给BlockManagerMasterEndPoint终端点；

​		2.在该终端点接受到消息时，从blockLocations列表中找出该RDD对应的数据存在BlockManagerId列表，查询完毕后，更新blockLocations和blockManagerInfo两个数据块元数据列表；

​		3.把获取的BlockMnagerId类别，发送消息给所在BlockMangerSlaveEndPoint终端点，通知其删除该Executor上的RDD,删除时调用BlockManager的removeRdd方法，删除在Executor上RDD所对应的数据块。



​		我们来看一下Spark存储模块类之间的关系。在整个模块中BlockManager是其核心，他不仅提供存储模块处理各种存储方式的读写方法，而且为Shuffle模块提供数据处理等操作接口。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gd4hahjbj30y40jmgus.jpg" alt="image-20190929141754804" style="zoom:60%;" />

​	BlockManager存在Driver端和每个Executor中，在Driver端的BlockManager**保存了数据的元数据信息**，而在Executor的BlockManager**根据接收到的消息进行操作**：

- 当Executor的BlockManager接收到读取数据，根据数据块所在节点是否本地使用BlockManager不同方法处理。如果在本地，直接调用MemoryStroe和DiskStore中的getValues、getBytes进行读取；如果在远程，则调用BlockTransferService的服务进行获取远程节点上的数据。

- 当Executor的BlockManager接收到写取数据，如果不需要创建副本，则调用BlockStore的接口方法进行处理。



### 1.2 存储级别

​		可以使用persist或cache方法显示的将RDD的数据缓存在内存或者磁盘中。

​		RDD第一次被计算时，persist方法会根据参数StorageLeve的设置采取特定的缓存策略，当RDD原本存储级别为NONE或者新传递进来的存储级别值与原来的存储级别相等时才进行操作。

​		persist操作是控制操作的一种，它只改变原RDD的元数据信息，并没有进行数据的存储操作，真正进行是在RDD的iteraor方法中。

​		在StorageLevel类中，根据useDisk、useMemory、useOffHeap、deserialized、replication 5个参数的组合，提供12种存储级别的缓存策略。



### 1.3 RDD存储调用

​		RDD包含多个Partition，每个Partition对应**一个数据块Block**，那么**每个RDD包含一个或者多个数据块**，每个数据块拥有一个唯一的编号BlockId，对应数据块编号规则为：rdd_ + rddId + _+splitIndex , 其中splitIndex为该数据块对应Partition的序列号。(一个partition对应一个block)

调用iterate方法时，先根据数据块Block编号判断是否已经按照指定的存储级别进行存储，

如果存在该数据块Block，则从本地或远程节点读取数据；

如果不存在该数据块Block，则调用RDD的计算方法得出结果，并把结果按照指定的存储级别进行存储。



### 1.4 读数据过程

​		BlockManager的get方法是读数据的入口点，在读取时分为**本地读取**和**远程节点读取**两个步骤。

​		本地读取使用getLocalValues方法，在该方法中根据不同的存储级别直接调用不同存储的实现。

​		远程节点读取使用getRemoteValues方法，在getRemoteValues方法中调用了getRemoteBytes方法，在方法中调用远程数据传输服务类BlockTransferService的fetchBlockSync进行处理，使用Netty的fetchBlocks方法获取数据。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7ggv0d2d2j30uu0p07el.jpg" alt="image-20190929162717374" style="zoom:60%;" />

在本地读取的时候，根据不同的存储级别可以分为内存和磁盘两种读取方式。

（1）内存读取

​		getLocalValues方法中，读取内存中数据根据返回的是封装成BlockResult类型还是数据流，分别调用**MemoryStore**的getValues和getBytes两种方法。



（2）磁盘读取

​		磁盘读取在getLocalValues方法中，调用的是**DiskStore**的getBytes方法，在读取磁盘中的数据后需要把这些数据缓存到内存中。

​		在spark中由spark.local.dir设置磁盘存储的一级目录，默认情况下设置1个一级目录，在1级目录下最多创建64个二级目录。一级目录命名为spark-UUID.randomUUID,其中，randomUUID为16位的UUID，二级目录以数据命名，范围是00~63。目录中文件的名字是数据块的名称blockId.name

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7gijt9l16j30uu0bcjuy.jpg" alt="image-20190929172545541" style="zoom:60%;" />

​		在DiskStore中的getBytes方法中，调用DiskBlockManager的getFile方法获取数据块所在文件的句柄。

​		获取文件句柄后，读取整个文件内容，以RandomAccessFile的只读方式打开该文件。

​		在远程节点读取数据的时候，Spark提供了Netty远程读取方式。

​		（1）Spark远程读取数据入口为getRemoteValues，然后调用getRemoteBytes方法，在该方法中调用getLocations方法向BlockManagerMasterEndPoint终端点发送**getLocations消息**，请求数据块所在的位置信息。当Driver的终端点收到请求消息时，根据数据块的编号获取该数据块所在的位置列表，根据是否是本地节点数据对**位置列表进行排序**。

​		获取数据块的位置列表后，在BlockManager.getRemoteBytes方法中调用BlockTransferService提供的fetchBlockSync方法进行读取远程数据。

​		（2）调用远程数据传输服务BlockTransferService的fetchBlockSync方法后，在该方法中继续调用fetchBlocks方法。该方法是一个抽象方法，实际上调用的是Netty远程数据服务NettyBlockTransferService类中的fetchBlocks方法。在fetchBlocks方法中，根据远程节点的地址和端口创建通信客户端TransportClient，通过该RPC客户端指定节点发送读取数据消息。

​		（3）当远程节点的RPC服务端接受到客户端发送消息时，在NettyBlockRpcServer类中对消息进行匹配。如果是请求读取消息时，则调用BlockManager的**getBlockData**方法读取该节点上的数据，读取的数据块封装为ManagerBuffer序列缓存在内存中，然后使用Netty提供的传输通道，把数据传输到请求节点上，完成远程传输任务。



### 1.5 写数据过程

​		BlockManager的doPutIterator方法是写数据的入口点。在该方法中，根据数据是否缓存在内存中进行处理。如果不缓存在内存中，则调用BlockManager的putIterator方法直接存储磁盘。如果缓存到内存中，需要先判断数据存储级别是否进行了反序列化。

​		在写入数据完成时，一方面把数据块的**元数据发送给Driver端**的BlockManagerMasterEndpoint终端点，请求其更新数据元数据，另一方面判断是否需要**创建数据副本**，如果需要则调用replicate方法，把数据写到远程节点上。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7halftpenj30sc0pgqco.jpg" alt="image-20190930093605698" style="zoom: 60%;" />



**1.写入内存**

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7hanep1nej30su0hm79x.jpg" alt="image-20190930093759495" style="zoom: 50%;" />

​		图中下半部分为已经使用的内存，在这些内存中存放在entries中，entries是由不同数据块生成的MemoryEnty构成。

​		图中上半部分为可用内存，这些内存用于尝试展开数据块，这些展开数据块的线程并不是一下子把数据展开到内存中，而是采取”步步为营“的策略，在每个步中都会先检查内存大小是否足够，如果内存大小不足，则尝试把内存中的数据写入到磁盘中，需要释放空间用来存放新写入的数据。



**2.写入磁盘**

​	Spark写入磁盘的方法调用了DiskStore的put方法，该方法提供了写入文件的回调方法writeFunc。在该方法中先获取写入文件句柄，然后把数据序列化为数据流，最后根据回调方法把数据写入文件中。



## 2.shuffle分析

### 2.1 shuffle简介

Spark与Hadoop遇到的情况类似，在Shuffle过程中存在如下问题：

- 数据量非常大，达到TB或PB级别。这些数据分散到数百甚至数千的集群中运行，如果管理为后续任务创建数据众多的文件，以及处理大小超过内存的数据量？

- 如何对结果进行序列化和反序列化，以及在传输之前如何进行压缩处理？

  

### 2.2 shuffle的写操作

**（1）基于hash的Shuffle写操作**

​	该机制中每一个Mapper会根据Reduce的数量创建出相应的**bucket**，bucket的数量是M*R，其中M是Map的个数，R是Reduce的个数；mapper生成的结果会根据设置的Partition算法填充到每个bucket中。这里面的bucket是个抽象的概念。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7hbt0mr3jj30xy0ekgsx.jpg" alt="image-20190930101758926" style="zoom:50%;" />

​		Spark并不在Reduce端做merge sort，而是使用聚合。聚合实际上是一个HashMap，它以当前任务输出结果的key为键，以任意要combine类型为值，当在WordCount的Reduce进行单词计算时，它会将Shuffle读到的每一个键值对更新或者插入到HashMap中。这样就不需要预先把所有的键值对进行merge sort，而是来一个处理一个，省下了外部排序这个步骤。



**（2）基于排序的shuffle写操作**

​	基于hash的Shuffle写操作能够较好的完成Shuffle数据写入，但存在两大问题：

- 若当前任务数S为1000，后续任务数F为1000，那么会产生1M个文件，这对于文件系统是一个非常大的负担。
- 每个WriteHandler 默认需要100KB的内存，内存需要C * F * 100KB，其中C是Spark集群中运行的核数，F为后续任务数据，如果后续任务很大的话，缓存所占用的内存是一笔不小的开销。

​       基于排序的shuffle写操作机制，每个Shuffle Map Task不会为后续的每个人物创建单独的文件，而是会将所有的结果写到同一个文件中，对应的生成一个Index文件进行索引。

SortShuffleWriter的write方法基本原理如下：

​		先判断Shuffle Map Task输出结果在Map端是否需要合并，如果需要合并，则外部排序中进行聚合并排序；如果不需要排序，则外部排序中不进行聚合和排序，例如sortByKey操作在Reduce端进行聚合排序。

​		确定好外部排序方式后，在外部排序中使用PartitionAppendOnlyMap来存放数据，当占用的内存已经超越了阈值，则将Map中的内容溢写到磁盘中，每次溢写产生一个不同的文件。当所有数据处理完毕后，一部分计算结果在内存中，另一部分计算结果溢写到一个或者多个文件中，这时通过merge操作将内存和spill文件中内容合并到一个文件中。



<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7hch6bwytj30xe0jmtkf.jpg" alt="image-20190930104106000" style="zoom:50%;" />



### 2.3 shuffle的读操作

读取数据需要解决2个问题：

- Shuffle写有基于哈希和排序两种方式，他们对应读取方式如何？
- 如何确认下游任务读取数据的位置信息，位置信息包括**所在节点**、**Executor编号**和读取数据块序列等？

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7qlranfeej30xe0jm11m.jpg" alt="image-20191008105216375" style="zoom:60%;" />

（1）在SparkEnv启动时，会对ShuffleManager、BlockManager和MapOutputTracker等实例化。ShuffleManager配置项有HashShuffleManager、SortShuffleManager和自定的ShuffleManager等3种选项。

（2）在BlockStoreShuffleReader的read方法中，调用MapOutputTracker的getMapSizesByExecutorId方法。

（3）知道Shuffle结果的位置信息后，对这些位置进行筛选，判断当前运行的数据是从本地还是从远程节点获取。

（4）读取数据后，判断shuffleDependency是否定义聚合，如果需要，则根据键值进行聚合。

下面从代码分析Shuffle读的实现：

（1）Shuffle读的起始点是由ShuffleRDD.computer发起的，在该方法中会调用ShuffleManager的getReader方法，**基于哈希和排序Shuffle读都是使用了HashShuffleReader**的reader方法。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t4h23fvhj30xm0ni161.jpg" alt="image-20191008105503003" style="zoom:67%;" />

（2）在HashShuffleReader的read方式中先实例化ShuffleBlockFetcherIterator，在该实例化过程中，通过MapOutputTracker的**getMapSizesByExecutorId**获取上游ShuffleMapTask输出数据的**元数据**。先尝试在本地的mapStatues获取，如果获取不到，则通过RPC通信框架发送消息。

（3）获取读取数据的位置信息后，返回到ShuffleBlocFetcherIterator的initialize方法。在该方法中先通过调用splitLocalRemoteBlocks方法对获取的数据位置信息进行分区，判断数据所处的位置是本地节点还是远程节点。如果是**本地节点使用fetchLocalBlocks方法**获取数据。如果是远程节点使用fetchUpToMaxBytes方法。

（4）数据读取完毕后，回到BlockStoreShuffleReader的read方法，判断是否定义聚合，如果需要，则根据键值调用Aggregator的combineCombinersByKey方法进行聚合。聚合完毕，使用外部排序对数据进行排序并放入内存。

（5）读取数据后，判断ShuffleDependency是否定义聚合（Aggregation），如果需要，则根据键值进行聚合。



## 3.序列化和压缩

​	序列化能够进行数据压缩等操作，通过压缩能减少内存占用以及IO和网络数据传输开销，提升Spark整体的应用性能。



### 3.1 序列化

​	内置了2个数据序列化类：JavaSerializer和KryoSerializer，这个两个类继承于抽象类Serializer，在SparkSql中SparkSqlSerializer继承于KryoSerializer，关系如下图：

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t2uptvphj30xi0hc442.jpg" alt="image-20191010141457314" style="zoom:50%;" />

​		默认情况下使用JavaSerializer序列方法，它使用的是Java的ObjectOutputStream序列化框架。虽然灵活，但是它的性能不佳。

​		这里可配置的序列化对象是Shuffle数据以及RDD缓存场合，对于Spark**任务的序列化**是通过spark.closure.serializer来配置，目前只支持JavaSerializer。



### 3.2 压缩

​	提供3种压缩方法，分别是LZ4、 LZF、 Snappy，这三个方法均继承于特质类CompressionCodec，并实现了其压缩和解压两个方法。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t3317w8vj30xi0cm79e.jpg" alt="image-20191010142255012" style="zoom:50%;" />



​	Snappy提供了更高的**压缩速度**，LZF提供了更高的压缩比，LZ4提供了压缩速度和压缩俱佳的性能。



## 4.共享变量

​		当一个函数传递给远程集群节点上运行的spark操作时，该函数中所有的变量都会在各节点上创建副本，在各节点中的变量相互隔离并由所在节点的函数进行调用，并且这些变量的更新都不会传递给Driver程序。任务间通用、可读写的共享变量是低效的。然后spark提供了2种类型共享变量：广播变量和累加器。



### 4.1 广播变量

​		广播变量允许开发人员在每个节点缓存**只读的变量**，而不是在任务之间传递这些变量。当多个调度阶段需要相同的数据，显示地创建广播变量才有用。

​		可以通过SparkContext.boardcast(v)创建一个广播变量v，该广播变量封装在v变量中，可使用.value方法进行访问。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t3ny4cecj30y805cdhx.jpg" alt="image-20191010144301349" style="zoom:50%;" />

为了确保所有节点获得相同的变量，**对象v广播后只读不能够被修改**。



### 4.2 累加器

​		累加器是Spark中仅有通过关联操作进行累加的变量，因此能够有效的支持并行计算，它们能够用于计数和求和。Spark原生支持数值类型的累加器，不过开发人员能够定义新的类型。

​		通过调用SparkContext.accumulator(v)方法初始化累加器变量v，在集群中的任务能够使用加法或者+=操作进行累加器操作。然而不能再应用程序中读取这些值，只能由Driver程序通过读方法获取这些累加器的值。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t3srd5vhj30y009u41c.jpg" alt="image-20191010144743244" style="zoom:60%;" />

​		累加器**只能由Spark内部进行更新**，并保证每个人物在累加器的更新操作仅执行一次，也就是说，重启任务也不应该更新。

​		累加器同样具有懒加载的求值模型。如果在RDD操作中进行更新，他们的值只在RDD进行行动操作时才进行更新。



## 5.实例演示

运行代码：

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7t44u718gj30zs07owj3.jpg" alt="image-20191010145916149" style="zoom:60%;" />

（1）从HDFS读取数据

（2）对读取数据进行清洗

（3）获取年份和每日最高温度字段进行Map操作

（4）得到所有年份最高温度降序排序



**结论：**

- 基于哈希的Shuffle写操作，Shuffle运行过程中所创建的文件数是S*F，其中S为ShuffleMap Task的任务数，而F是后续任务的任务数。
- 基于排序的Shuffle写操作，shuffle运行过程中Map Task不会为后续的每个任务创建单独的文件，而是会将所有的结果写到同一个文件中，对应生成一个Index文件进行索引。







