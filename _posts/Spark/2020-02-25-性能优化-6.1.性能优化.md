

## 1. 代码层面

### 1. 对多次使用的 RDD 进行持久化并序列化

原因:Spark 中对于一个 RDD 执行多次算子的默认原理是这样的:每次对一 个 RDD 执行一个算子操作时，都会重新从源头出计算一遍，计算出那个 RDD 来，然后再对这个 RDD 执行你的算子操作。这种方式的性能是很差的。

解决办法:因此对于这种情况，建议是对多次使用的 RDD 进行持久化。此 时 spark 就会根据你的持久化策略，将 RDD 中的数据保存到内存或者磁盘中。 以后每次对这个 RDD 进行算子操作时，都会直接从内存或磁盘中提取持久化的 RDD 数据，然后执行算子，而不会从源头出重新计算一遍这个 RDD。 



### 2. 广播大数据集

 有时会遇到在算子函数中使用外部变量的场景，建议使用 spark 的广播功能

来提升性能。默认情况下，算子函数使用外部变量时会将该变量复制多个副本， 通过网络传输到 task 中，**此时每个 task 都有一个变量副本。**如果变量本身比较大，那么大量的变量副本在网络中传输的性能开销以及在各个节点的 executor 中占用过多的内存导致频繁 GC，都会极大影响性能。所以建议使用 spark 的广播性能，对该变量进行广播。广播的好处在于，会保证每个 executor 的内存中， 只驻留一份变量副本，而 executor 中的 task 执行时会共享该 executor 中的那份变量副本。这样的话，可以大大降低变量副本的数量，从而减少网络传输的性能开销，并减少对 executor 内存的占用开销，降低 GC 的频率。



###  3. 批次和窗口大小的设置(针对 spark streaming 中的特殊优化)

​		最常见的问题是 Spark Streaming 可以使用的最小批次间隔是多少。寻找最小批次大小的最佳实践是从一个比较大的批次开始，不断使用更小的批次大小。 如果 streaming 用户界面中显示的处理时间保持不变，那么就可以进一步减小批次大小。对于窗口操作，计算结果的间隔对于性能也有巨大的影响。



## 2. 运行层面

###  1. 提高并行度

减少批处理所消耗时间的常见方式还有提高并行度。

首先可以增加接收器数目，当记录太多导致但台机器来不及读入并分发的话，接收器会成为系统瓶颈，这时需要创建多个输入 DStream 来增加接收器数目，然后使用 union 来把数据合并为一个数据源;然后可以将接收到的数据显式的重新分区，如果接收器数目无法在增加，

可以通过使用 DStream.repartition 来显式重新分区输入流来重新分配收到的数据;

最后可以提高聚合计算的并行度，对于像 reduceByKey()这样的操作，可以在第二个参数中制定并行度。



### 2. 数据本地化

​		数据本地化对于 spark job 性能有着巨大的影响。如果数据以及要计算它的代码是在一起的，那么性能自然会高。但是如果数据和计算它的代码是分开的，那么其中之一必须要另 外一方的机器上。通常来说，移动代码到其他节点，会比移动数据到所在节点上去，速度要 快的多，因为代码比较小。Spark 也正是基于整个数据本地化的原则来构建 task 调度算法的。 数据本地化，指的是数据距离它的代码有多近。基于数据距离代码的距离，有几种数据本地 化级别:

- (a) PROCESS_LOCAL:数据和计算它的代码在同一个 JVM 进程里面; 

- (b) NODE_LOCAL:数据和计算它的代码在一个节点上，但是不在一个进程中，比如不在同 一个 executor 进程中，或者是数据在 hdfs 文件的 block 中;

-  (c) NO_PREF:数据从哪里过来，性能都是一样的;

-  (d) RACK_LOCAL:数据和计算它的代码在一个机架上;

-  (e) ANY:数据可能在任意地方，比如其他网络环境内，或者其他机架上。

​      Spark 倾向于使用最好的本地化级别来调度 task，但是这是不可能的。如果没有任何未处理 的数据在空闲的 executor 上，那么 Spark 就会放低本地化级别。

这时有两个选择:

第一，等待，直到 executor 上的 cpu 释放出来，那么就分配 task 过去;

第二，立即在任意一个 executor 上启动一个 task。Spark 默认会等待一会，来期望 task 要处理的数据所在的节点上的 executor 空闲出一个 cpu，从而将 task 分配过去。只要超过了时间，那么 spark 就会将 task分配到其 他任意一个空闲的 executor 上。

可以设置参数，spark.locality 系列参数，来调节 spark 等待 task 可以进行数据本地化的时间。
saprk.locality.wait(3000ms) 、 spark.locality.wait.node 、 spark.locality.wait.process 、 spark.locality.wait.rack。




### 3. 垃圾回收调优

首先使用更高效的数据结构，比如 array 和 string;

其次是在持久化 rdd 时，使用序列化的持久化级别，而且使用 Kryo 序列化 类库;这样每个 partition 就只是一个对象(一个字节数组)

然后是监测垃圾回收，可以通过在 spark-submit 脚本中，增加一个配置即可 --conf"spark.executor.extraJavaOptions=-verbose:gc-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"

> 注意，这里打印出 java 虚拟机的垃圾回收的相关信息，但是输出到了 worker 上的日志，而不是 driver 日志上。还可以通过 sparkUI(4040 端口)来观察每个 stage 的垃圾回收的情况;

​		然后，优化 executor 内存比例。对于垃圾回收来说，最重要的是调节 RDD 缓存占用的内存空间，与算子执行时创建对象占用的内存空间的比例。默认是 60%存放缓存 RDD，40%存放 task 执行期间创建的对象。出现的问题是，task 创建的对象过大，一旦发现 40%内存不够用了，就会频繁触发 GC 操作，从而频 繁导致 task 工作线程停止，降低 spark 程序的性能。解决措施是调优这个比例， 使用 new SparkConf().set("spark.storage.memoryFraction", "0.5")即可，给年轻代更多的空间来存放短时间存活的对象。

​		最后，如果发现 task 执行期间大量的 Full GC 发生，那么说明年轻代的 Eden 区域给的空间不够大，可以执行以下操作来优化垃圾回收行为:给 Eden 区域分 配更大的空间，使用-Xmn 即可，通常建议给 Eden 区域预计大小的 4/3;

​		如果使用 hdfs 文件，那么很好估计 Eden 区域大小。如果每个 executor 有 4 个 task，然后每个 hdfs 压缩块解压后大小是 3 倍，此外每个 hdfs 块的大小是 64M， 那么 Eden 区域的预计代销就是 4*3*64MB，通过-Xmn 参数，将 Eden 区域大小 设置为 4*3*64*4/3。









