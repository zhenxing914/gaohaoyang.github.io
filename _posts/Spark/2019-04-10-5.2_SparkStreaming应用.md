---

layout: post
title:  "5.SparkStreaming应用"
categories: "Spark"
tags: "Spark SparkStreaming"
author: "songzhx"
date:   2019-04-10 15:42:00 
---



spark日志收集器配置

```scala
package cn.itcast.spark.day5

import org.apache.log4j.{Logger, Level}
import org.apache.spark.Logging

object LoggerLevels extends Logging {

  def setStreamingLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
  }
}
```



### 1. 简单worldcount

   ```scala
   package cn.itcast.spark.day5
   
   import org.apache.spark.{SparkConf, SparkContext}
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   
   /**
     * Created by root on 2016/5/21.
     */
   object StreamingWordCount {
   
     def main(args: Array[String]) {
   
       LoggerLevels.setStreamingLogLevels()
       //StreamingContext
       val conf = new SparkConf().setAppName("StreamingWordCount").setMaster("local[2]")
       val sc = new SparkContext(conf)
       val ssc = new StreamingContext(sc, Seconds(5))
       //接收数据
       val ds = ssc.socketTextStream("172.16.0.11", 8888)
       //DStream是一个特殊的RDD
       //hello tom hello jerry
       val result = ds.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_)
       //打印结果
       result.print()
       ssc.start()
       ssc.awaitTermination()
     }
   }
   
   ```

   

### 2. 实现叠加的worldcount - updateStateByKey   

   ```scala
   package cn.itcast.spark.day5
   
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   import org.apache.spark.{HashPartitioner, SparkConf, SparkContext}
   
   /**
     * Created by root on 2016/5/21.
     */
   object StateFulWordCount {
   
   
     //Seq这个批次某个单词的次数
     //Option[Int]：以前的结果
   
     //分好组的数据
     val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) => {
       //iter.flatMap(it=>Some(it._2.sum + it._3.getOrElse(0)).map(x=>(it._1,x)))
       //iter.map{case(x,y,z)=>Some(y.sum + z.getOrElse(0)).map(m=>(x, m))}
       //iter.map(t => (t._1, t._2.sum + t._3.getOrElse(0)))
       iter.map{ case(word, current_count, history_count) => (word, current_count.sum + history_count.getOrElse(0)) }
     }
   
     def main(args: Array[String]) {
       LoggerLevels.setStreamingLogLevels()
       //StreamingContext
       val conf = new SparkConf().setAppName("StateFulWordCount").setMaster("local[2]")
       val sc = new SparkContext(conf)
       //updateStateByKey必须设置setCheckpointDir
       sc.setCheckpointDir("c://ck")
       val ssc = new StreamingContext(sc, Seconds(5))
   
       val ds = ssc.socketTextStream("172.16.0.11", 8888)
       //DStream是一个特殊的RDD
       //hello tom hello jerry
       val result = ds.flatMap(_.split(" ")).map((_, 1)).updateStateByKey(updateFunc, new HashPartitioner(sc.defaultParallelism), true)
   
       result.print()
   
       ssc.start()
   
       ssc.awaitTermination()
   
     }
   }
   
   ```

   

### 3. Flume push 模式

   ```scala
   package cn.itcast.spark.day5
   
   import org.apache.spark.SparkConf
   import org.apache.spark.streaming.flume.FlumeUtils
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   
   /**
     * Created by ZX on 2015/6/22.
     */
   object FlumePushWordCount {
   
     def main(args: Array[String]) {
       val host = args(0)
       val port = args(1).toInt
       LoggerLevels.setStreamingLogLevels()
       val conf = new SparkConf().setAppName("FlumeWordCount")//.setMaster("local[2]")
       val ssc = new StreamingContext(conf, Seconds(5))
       //推送方式: flume向spark发送数据
       val flumeStream = FlumeUtils.createStream(ssc, host, port)
       //flume中的数据通过event.getBody()才能拿到真正的内容
       val words = flumeStream.flatMap(x => new String(x.event.getBody().array()).split(" ")).map((_, 1))
   
       val results = words.reduceByKey(_ + _)
       results.print()
       ssc.start()
       ssc.awaitTermination()
     }
   }
   
   ```

   

### 4. Flume poll 模式

   ```scala
   package cn.itcast.spark.day5
   
   import java.net.InetSocketAddress
   
   import org.apache.spark.SparkConf
   import org.apache.spark.storage.StorageLevel
   import org.apache.spark.streaming.flume.FlumeUtils
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   
   object FlumePollWordCount {
     def main(args: Array[String]) {
       val conf = new SparkConf().setAppName("FlumePollWordCount").setMaster("local[2]")
       val ssc = new StreamingContext(conf, Seconds(5))
       //从flume中拉取数据(flume的地址)
       val address = Seq(new InetSocketAddress("172.16.0.11", 8888))
       val flumeStream = FlumeUtils.createPollingStream(ssc, address, StorageLevel.MEMORY_AND_DISK)
       val words = flumeStream.flatMap(x => new String(x.event.getBody().array()).split(" ")).map((_,1))
       val results = words.reduceByKey(_+_)
       results.print()
       ssc.start()
       ssc.awaitTermination()
     }
   }
   
   ```

-  基于 Receiver-based 的 createStream 方法

Receiver接收固定时间间隔的数据（放在内存中），达到固定的时间才进行处理，使用Kafka高级API：createStream，自动维护偏移量，效率低并且容易丢数据。
(Kafka broker version 0.10.0 or higher 已废弃Receiver方式)

- Direct Approach 方式的 createDirectStream 方法

Direct直连方式，相当于直接连接到Kafka的分区上，使用Kafka底层API：createDirectStream，效率高，需要我们自己维护偏移量。

### 5. Kafka简单模式

   ```scala
   package cn.itcast.spark.day5
   
   import org.apache.spark.storage.StorageLevel
   import org.apache.spark.{HashPartitioner, SparkConf}
   import org.apache.spark.streaming.kafka.KafkaUtils
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   
   /**
     * Created by root on 2016/5/21.
     */
   object KafkaWordCount {
   
     val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) => {
       //iter.flatMap(it=>Some(it._2.sum + it._3.getOrElse(0)).map(x=>(it._1,x)))
       iter.flatMap { case (x, y, z) => Some(y.sum + z.getOrElse(0)).map(i => (x, i)) }
     }
   
   
     def main(args: Array[String]) {
       LoggerLevels.setStreamingLogLevels()
       val Array(zkQuorum, group, topics, numThreads) = args
       val sparkConf = new SparkConf().setAppName("KafkaWordCount").setMaster("local[2]")
       val ssc = new StreamingContext(sparkConf, Seconds(5))
       ssc.checkpoint("c://ck2")
       //"alog-2016-04-16,alog-2016-04-17,alog-2016-04-18"
       //"Array((alog-2016-04-16, 2), (alog-2016-04-17, 2), (alog-2016-04-18, 2))"
       val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
       val data = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap, StorageLevel.MEMORY_AND_DISK_SER)
       val words = data.map(_._2).flatMap(_.split(" "))
       val wordCounts = words.map((_, 1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)
       ssc.start()
       ssc.awaitTermination()
     }
   }
   
   ```

   

### 6. Kafka Direct模式

   ```scala
   package cn.itcast.spark.day5
   
   import kafka.serializer.StringDecoder
   import org.apache.log4j.{Level, Logger}
   import org.apache.spark.SparkConf
   import org.apache.spark.rdd.RDD
   import org.apache.spark.streaming.kafka.{KafkaManager, KafkaUtils}
   import org.apache.spark.streaming.{Seconds, StreamingContext}
   
   
   object DirectKafkaWordCount {
   
     /*  def dealLine(line: String): String = {
         val list = line.split(',').toList
     //    val list = AnalysisUtil.dealString(line, ',', '"')// 把dealString函数当做split即可
         list.get(0).substring(0, 10) + "-" + list.get(26)
       }*/
   
     def processRdd(rdd: RDD[(String, String)]): Unit = {
       val lines = rdd.map(_._2)
       val words = lines.map(_.split(" "))
       val wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)
       wordCounts.foreach(println)
     }
   
     def main(args: Array[String]) {
       if (args.length < 3) {
         System.err.println(
           s"""
              |Usage: DirectKafkaWordCount <brokers> <topics> <groupid>
              |  <brokers> is a list of one or more Kafka brokers
              |  <topics> is a list of one or more kafka topics to consume from
              |  <groupid> is a consume group
              |
           """.stripMargin)
         System.exit(1)
       }
   
       Logger.getLogger("org").setLevel(Level.WARN)
   
       val Array(brokers, topics, groupId) = args
   
       // Create context with 2 second batch interval
       val sparkConf = new SparkConf().setAppName("DirectKafkaWordCount")
       sparkConf.setMaster("local[*]")
       sparkConf.set("spark.streaming.kafka.maxRatePerPartition", "5")
       sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
   
       val ssc = new StreamingContext(sparkConf, Seconds(2))
   
       // Create direct kafka stream with brokers and topics
       val topicsSet = topics.split(",").toSet
       val kafkaParams = Map[String, String](
         "metadata.broker.list" -> brokers,
         "group.id" -> groupId,
         "auto.offset.reset" -> "smallest"
       )
   
       val km = new KafkaManager(kafkaParams)
   
       val messages = km.createDirectStream[String, String, StringDecoder, StringDecoder](
         ssc, kafkaParams, topicsSet)
   
       messages.foreachRDD(rdd => {
         if (!rdd.isEmpty()) {
           // 先处理消息
           processRdd(rdd)
           // 再更新offsets
           km.updateZKOffsets(rdd)
         }
       })
   
       ssc.start()
       ssc.awaitTermination()
     }
   }
   
   ```

   因为引用的源码类私有化设置-只有kafka包可以访问，所以需要创建kafka内部包名一样的包。

   ```scala
   package org.apache.spark.streaming.kafka
   
   import kafka.common.TopicAndPartition
   import kafka.message.MessageAndMetadata
   import kafka.serializer.Decoder
   import org.apache.spark.SparkException
   import org.apache.spark.rdd.RDD
   import org.apache.spark.streaming.StreamingContext
   import org.apache.spark.streaming.dstream.InputDStream
   import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset
   
   import scala.reflect.ClassTag
   
   /**
     * 自己管理offset
     */
   class KafkaManager(val kafkaParams: Map[String, String]) extends Serializable {
   
     private val kc = new KafkaCluster(kafkaParams)
   
     /**
       * 创建数据流
       */
     def createDirectStream[K: ClassTag, V: ClassTag, KD <: Decoder[K]: ClassTag, VD <: Decoder[V]: ClassTag](
                                                                                                               ssc: StreamingContext, kafkaParams: Map[String, String], topics: Set[String]): InputDStream[(K, V)] =  {
       val groupId = kafkaParams.get("group.id").get
       // 在zookeeper上读取offsets前先根据实际情况更新offsets
       setOrUpdateOffsets(topics, groupId)
   
       //从zookeeper上读取offset开始消费message
       val messages = {
         val partitionsE = kc.getPartitions(topics)
         if (partitionsE.isLeft)
           throw new SparkException(s"get kafka partition failed: ${partitionsE.left.get}")
         val partitions = partitionsE.right.get
         val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
         if (consumerOffsetsE.isLeft)
           throw new SparkException(s"get kafka consumer offsets failed: ${consumerOffsetsE.left.get}")
         val consumerOffsets = consumerOffsetsE.right.get
         KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)](
           ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) => (mmd.key, mmd.message))
       }
       messages
     }
   
     /**
       * 创建数据流前，根据实际消费情况更新消费offsets
       * @param topics
       * @param groupId
       */
     private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = {
       topics.foreach(topic => {
         var hasConsumed = true
         val partitionsE = kc.getPartitions(Set(topic))
         if (partitionsE.isLeft)
           throw new SparkException(s"get kafka partition failed: ${partitionsE.left.get}")
         val partitions = partitionsE.right.get
         val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions)
         if (consumerOffsetsE.isLeft) hasConsumed = false
         if (hasConsumed) {// 消费过
           /**
             * 如果streaming程序执行的时候出现kafka.common.OffsetOutOfRangeException，
             * 说明zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。
             * 针对这种情况，只要判断一下zk上的consumerOffsets和earliestLeaderOffsets的大小，
             * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时,
             * 这时把consumerOffsets更新为earliestLeaderOffsets
             */
           val earliestLeaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)
           if (earliestLeaderOffsetsE.isLeft)
             throw new SparkException(s"get earliest leader offsets failed: ${earliestLeaderOffsetsE.left.get}")
           val earliestLeaderOffsets = earliestLeaderOffsetsE.right.get
           val consumerOffsets = consumerOffsetsE.right.get
   
           // 可能只是存在部分分区consumerOffsets过时，所以只更新过时分区的consumerOffsets为earliestLeaderOffsets
           var offsets: Map[TopicAndPartition, Long] = Map()
           consumerOffsets.foreach({ case(tp, n) =>
             val earliestLeaderOffset = earliestLeaderOffsets(tp).offset
             if (n < earliestLeaderOffset) {
               println("consumer group:" + groupId + ",topic:" + tp.topic + ",partition:" + tp.partition +
                 " offsets已经过时，更新为" + earliestLeaderOffset)
               offsets += (tp -> earliestLeaderOffset)
             }
           })
           if (!offsets.isEmpty) {
             kc.setConsumerOffsets(groupId, offsets)
           }
         } else {// 没有消费过
         val reset = kafkaParams.get("auto.offset.reset").map(_.toLowerCase)
           var leaderOffsets: Map[TopicAndPartition, LeaderOffset] = null
           if (reset == Some("smallest")) {
             val leaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)
             if (leaderOffsetsE.isLeft)
               throw new SparkException(s"get earliest leader offsets failed: ${leaderOffsetsE.left.get}")
             leaderOffsets = leaderOffsetsE.right.get
           } else {
             val leaderOffsetsE = kc.getLatestLeaderOffsets(partitions)
             if (leaderOffsetsE.isLeft)
               throw new SparkException(s"get latest leader offsets failed: ${leaderOffsetsE.left.get}")
             leaderOffsets = leaderOffsetsE.right.get
           }
           val offsets = leaderOffsets.map {
             case (tp, offset) => (tp, offset.offset)
           }
           kc.setConsumerOffsets(groupId, offsets)
         }
       })
     }
   
     /**
       * 更新zookeeper上的消费offsets
       * @param rdd
       */
     def updateZKOffsets(rdd: RDD[(String, String)]) : Unit = {
       val groupId = kafkaParams.get("group.id").get
       val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
   
       for (offsets <- offsetsList) {
         val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)
         val o = kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)))
         if (o.isLeft) {
           println(s"Error updating the offset to Kafka cluster: ${o.left.get}")
         }
       }
     }
   }
   
   
   ```

   引用文章：

   [spark streaming kafka1.4.1中的低阶api createDirectStream使用总结](https://blog.csdn.net/ligt0610/article/details/47311771)



### 7. 滑动窗口

StreamingContext：

第一参数为sparkContext对象，第二个参数为批次时间；

滑动窗口：

![这里写图片描述](https://tva1.sinaimg.cn/large/006tNbRwgy1gaoxoz0m9dj30i005p3za.jpg)

例如每个方块代表5秒钟,上面的虚线框住的是3个窗口就是15秒钟,这里的15秒钟就是窗口的长度,其中虚线到实线移动了2个方块表示10秒钟,这里的10秒钟就表示每隔10秒计算一次窗口长度的数据



   ```scala
   package cn.itcast.spark.day5
   
   import org.apache.spark.SparkConf
   import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}
   
   /**
     * Created by ZX on 2016/4/19.
     */
   object WindowOpts {
   
     def main(args: Array[String]) {
       LoggerLevels.setStreamingLogLevels()
       val conf = new SparkConf().setAppName("WindowOpts").setMaster("local[2]")
       val ssc = new StreamingContext(conf, Milliseconds(5000))
       val lines = ssc.socketTextStream("172.16.0.11", 9999)
       val pairs = lines.flatMap(_.split(" ")).map((_, 1))
       val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(15), Seconds(10))
       //Map((hello, 5), (jerry, 2), (kitty, 3))
       windowedWordCounts.print()
   //    val a = windowedWordCounts.map(_._2).reduce(_+_)
   //    a.foreachRDD(rdd => {
   //      println(rdd.take(0))
   //    })
   //    a.print()
   //    //windowedWordCounts.map(t => (t._1, t._2.toDouble / a.toD))
   //    windowedWordCounts.print()
   //    //result.print()
       ssc.start()
       ssc.awaitTermination()
     }
   
   }
   
   ```

   
