## 背景知识：

Spark总结整理(一)：Spark内核架构（Spark从启动到执行的流程）
Spark Streaming 总结整理(一)：Spark Streaming运行原理与核心概念



## 1. 明确 Spark中Job 与 Streaming中 Job 的区别
### 1.1 Spark Core
一个 RDD DAG Graph 可以生成一个或多个 Job（Action操作）

一个Job可以认为就是会最终输出一个结果RDD的一条由RDD组织而成的计算

Job在spark里应用里是一个被调度的单位



### 1.2 Streaming

一个 batch 的数据对应一个 DStreamGraph

而一个 DStreamGraph 包含一或多个关于 DStream 的输出操作

每一个输出对应于一个Job，一个 DStreamGraph 对应一个JobSet，里面包含一个或多个Job



## 2. Streaming Job的并行度

Job的并行度由两个配置决定：

```properties
spark.scheduler.mode(FIFO/FAIR)
spark.streaming.concurrentJobs
```


一个 Batch 可能会有多个 Action 执行，比如注册了多个 Kafka 数据流，每个Action都会产生一个Job

所以一个 Batch 有可能是一批 Job,也就是 JobSet 的概念

这些 Job 由 jobExecutor 依次提交执行

而 JobExecutor 是一个默认池子大小为1的线程池，所以只能执行完一个Job再执行另外一个Job

这里说的池子，大小就是由spark.streaming.concurrentJobs 控制的

**concurrentJobs 决定了向 Spark Core 提交Job的并行度**

提交一个Job，必须等这个执行完了，才会提交第二个

假设我们把它设置为2，则会并发的把 Job 提交给 Spark Core

Spark 有自己的机制决定如何运行这两个Job，这个机制其实就是FIFO或者FAIR（决定了资源的分配规则）

默认是 FIFO，也就是先进先出，把 concurrentJobs 设置为2，但是如果底层是FIFO，那么会优先执行先提交的Job

虽然如此，如果资源够两个job运行，还是会并行运行两个Job



## 3. spark.streaming.concurrentJobs 可让不同Batch的job同时在运行
Streaming 不仅仅能同时运行 同一个batch 的job，甚至还能同时运行不同 Batch的 Job



具体可参考：Spark Streaming 不同Batch任务可以并行计算么？





————————————————
版权声明：本文为CSDN博主「super_man_0820」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/super_wj0820/article/details/101775937