## 1. 累加器介绍

​		提供了将工作节点中的值聚合到驱动器程序中的简单语法。**累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数**。 假设我们在从文件中读取呼号列表对应的日志， 同时也想知道输入文件中有多少空行,下面的python程序使用累加器完成了这一点。

```python
file = sc.textFile(inputFile)
# 创建Accumulator[Int]并初始化为0
blankLines = sc.accumulator(0)
def extractCallSigns(line):
  global blankLines # 访问全局变量
  if (line == ""):
    blankLines += 1
  return line.split(" ")
callSigns = file.flatMap(extractCallSigns)
callSigns.saveAsTextFile(outputDir + "/callsigns")
print "Blank lines: %d" % blankLines.value
```

上面的例子中创建了一个叫作 blankLines 的 Accumulator[Int] 对象，然后在输入中看到一个空行时就对其加 1。执行完转化操作之后， 就打印出累加器中的值。注意，只有在运行 saveAsTextFile() 行动操作后才能看到正确的计数，因为行动操作前的转化操作flatMap() 是惰性的，所以作为计算副产品的累加器只有在惰性的转化操作 flatMap() 被saveAsTextFile() 行动操作强制触发时才会开始求值。

- 首先通过在驱动器中调用 SparkContext.accumulator(initialValue) 方法，创建出存有初始值的累加器。 返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值initialValue 的类型。
- Spark 闭包里的执行器代码可以使用累加器的 += 方法（在 Java 中是 add）增加累加器的值。
- 驱动器程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue()）来访问累加器的值

注意工作节点上的任务不能访问累加器的值（即不能读取累加器的值）。从这些任务的角度来看，累加器是一个只写变量。

 下面的示例来验证呼号器：

 内容：

```bash
{"address":"address here", "band":"40m","callsign":"KK6JLK","city":"SUNNYVALE",
"contactlat":"37.384733","contactlong":"-122.032164",
"county":"Santa Clara","dxcc":"291","fullname":"MATTHEW McPherrin",
"id":57779,"mode":"FM","mylat":"37.751952821","mylong":"-122.4208688735",...}
```



代码如下：

```python
# 创建用来验证呼号的累加器
validSignCount = sc.accumulator(0)
invalidSignCount = sc.accumulator(0)
def validateSign(sign):
  global validSignCount, invalidSignCount
  if re.match(r"\A\d?[a-zA-Z]{1,2}\d{1,4}[a-zA-Z]{1,3}\Z", sign):
    validSignCount += 1
    return True
  else:
    invalidSignCount += 1
    return False
# 对与每个呼号的联系次数进行计数
validSigns = callSigns.filter(validateSign)
contactCount = validSigns.map(lambda sign: (sign, 1)).reduceByKey(lambda (x, y): x
+ y)
# 强制求值计算计数
contactCount.count()
if invalidSignCount.value < 0.1 * validSignCount.value:
    contactCount.saveAsTextFile(outputDir + "/contactCount")
else:
    print "Too many errors: %d in %d" % (invalidSignCount.value, validSignCount.
value)
```



## 2. 累加器的容错性

- 对于要在行动操作中使用的累加器， Spark只会把每个任务对各累加器的修改应用一次。因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在 foreach() 这样的行动操作中。
- 转化操作中使用的累加器， 就不能保证有这种情况了。转化操作中累加器可能会发生不止一次更新。 举个例子，当一个被缓存下来但是没有经常使用的 RDD 在第一次从 LRU 缓存中被移除并又被重新用到时，这种非预期的多次更新就会发生。这会强制RDD根据其谱系进行重算， 而副作用就是这也会使得谱系中的转化操作里的累加器进行更新，并再次发送到驱动器中。**在转化操作中，累加器通常只用于调试目的。**



作者：Spike_3154
链接：https://www.jianshu.com/p/a0c6c8377c16
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。