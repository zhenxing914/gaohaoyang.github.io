---

layout: post
title:  "图解spark_6.Spark运行架构"
categories: "Spark"
tags: "Spark "
author: "songzhx"
date:   2019-10-10 16:22:00 
---

## 1.运行架构总体介绍

### 1.1 总体介绍

​		Spark虽然支持多种运行模式，但Spark应用程序的运行架构基本由三部分组成，包括SparkContext（驱动程序）、ClusterManager（集群资源管理器）、Executor（任务执行进程）组成。



### 1.2 重要类介绍



## 2. 本地运行模式

### 2.1  运行模式介绍

​		在本地模式中，Spark所有进程都运行在一台机器的JVM中。该运行模式一般用于测试。在运行中，如果在命令语句中不加任何配置，Spark默认设置为Local模式，本地模式的标准写法是local[N],**这里面的N表示的是打开N个线程进行多线程运行。**

​		在用户自己编写的程序中设置的运行模式的优先级要大于在Spark应用程序配置文件中参数设置的值。但是为了使应用程序能够更加灵活地部署在各种模式下，不建议把运行模式硬编码在代码中。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85tkqwmorj30kc0mstfm.jpg" alt="image-20191021135003140" style="zoom:50%;" />

1. 启动应用程序，在SparkContext启动过程中，初始化DAGScheduler和TaskSchedulerImpl两个调度器，同时初始化LocalBackend以及LocalEndPoint本地终端点。
2. 对作业进行划分调度阶段后，任务集按照拆分顺序发送任务到LocalEndPoint本地终端点，本地终端点收到任务集时在本地启动Executor，启动完毕后在Executor中执行任务集。
3. Executor执行任务状态通过LocalEndpoint本地终端点反馈给上层作业调度器，上层作业调度器根据收到信息更新任务状态。
4. 当应用程序完毕后进行回收资源，上层作业调度器注销在LocalBackend中运行的Executor，注销完毕释放DAGScheduler、TaskSchedulerImpl、LocalBackend等进程。



### 2.2 实现原理



## 3. 伪分布式运行模式

### 3.1 运行模式介绍

​		伪分布式运行顾名思义是在一台机器中模拟集群运行，相对独立模式中Master、Worker和SparkContext在不同节点上，伪分布式运行模式中这些进程都是在一台机器上。

​		下面演示模拟启动3个worker进程，每个Worker进程启动两个CPU核和1024MB内存来运行LocalPi。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85tkvc3pyj30zy01m759.jpg" alt="image-20191021141410708" style="zoom:50%;" />

​		伪分布式运行模式运行流程和独立运行模式相同，区别在于伪分布式运行模式运行在一个节点中。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85tkybel6j30wc0oitkd.jpg" alt="image-20191021141247837" style="zoom:50%;" />





### 3.2 实现原理



## 4. 独立运行模式

### 4.1 运行模式介绍

​		独立运行模式是Spark自身实现的资源调度框架，由客户端、Master节点和Worker节点组成，其中SparkContext即可以运行在Master节点上，也可以运行在本地客户端。当用Spark-Shell交互式工具提交作业或者直接使用run-example脚本来运行示例时，SparkContext运行在Master节点上；当使用Spark-Submit工具提交作业或者Idea等开发平台上运行Spark作业时，SparkContext运行在本地客户端。Worker节点可以通过ExecutorRunner运行在当前节点上的CoarseGrainedExecutorBackend进程。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85tl1hymoj310o0qogzx.jpg" alt="image-20191021142007110" style="zoom:50%;" />

1. 启动应用程序，在SparkContext启动过程中，先初始化DAGSheduler和TaskSchedulerImpl两个调度器，同时初始化SparkDeploySchedulerBackend，并在其内部启动终端点DriverEndpoint和ClientEndpoint。
2. 终端点ClientEndPoint向Master注册应用程序，Master收到注册消息把该应用加入到等待运行应用列表中，等待Master分配给该应用程序Worker。
3. 当应用程序获取到Worker时，Master会通知Worker中的终端点WorkerEndPoint创建CoarseGrainendExecutorBackend进程，在该进程中创建执行容器Executor。
4. Executor创建完毕后**发送消息给Master和终端点DriverEndpoint**，告知Executor已经创建完毕，在SparkContext成功注册后，等待接收从Driver终端点发送执行任务的消息。
5. SparkContext分配任务集给CoarseGrainedExecutorBackend执行,任务执行是在Executor按照一定调度策略进行的。
6. CoarseGrainedExecutorBackend在任务处理过程中，把处理任务的状态发送给SparkContext的终端点DriverEndpoint，SparkContext根据任务执行的不同的结果进行处理。
7. 应用程序运行完成后，SparkContext会进行资源回收，先销毁在Worker的CoarseGrainedExecutorBackend进程，然后注销其自身。



### 4.2 实现原理



## 5. Yarn运行模式

### 5.1 Yarn运行框架

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85rm2xow9j30we0nok2o.jpg" alt="image-20191021110841423" style="zoom:60%;" />



### 5.2 Yarn-client运行模式介绍

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85tl89j8oj311u0tk7o6.jpg" alt="image-20191021103144963" style="zoom: 50%;" />

1. 启动应用程序，在SparkContext启动过程中，初始化DAGsheduler调度器，使用反射方法初始化YarnScheduler和YarnClientShedulerBackend。YarnClientSchedulerBackend在内部启动终端点DriverEndPoint和client,然后Client向Yarn集群的ResourceManager申请启动ApplicationMaster。
2. ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求在这个Container中启动应用程序的Application Master。与Yarn-Cluster的区别是在该Application中不运行SparkContext，只与SparkContext进行联系进行资源的分配。
3. 客户端的SparkContext启动完毕后，与ApplicationMaster建立通信，向ResourceManager注册，根据任务信息向ResourceManager申请资源。
4. 一旦ApplicationMaster申请到资源，便与对应的NodeManager通信，要求它在获得的container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向SparkContext注册并申请任务集。
5. 客户端中SparkContext分配任务集给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行任务并向终端点EndPoint汇报运行的状态和进度。
6. 应用程序运行完成后，客户端的SparkContext向ResourceManager申请注销并关闭自身。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85rm84pitj30x605gq54.jpg" alt="image-20191021104753640" style="zoom:60%;" />

​	其中，“--maste”r参数默认是yarn-client，与Yarn-cluster模式不同的是，使用Yarn-Client模式提交应用程序，当运行结束之后可以直接在客户本地看到控制台打印的结果，这是因为SparkContext直接运行在客户端中。



### 5.3 Yarn-client运行模式实现原理





### 5.4 Yarn-Cluster运行模式介绍

​		在Yarn-Cluster模式中，当用户向Yarn提交一个应用程序后，Yarn将分为2个阶段，第一阶段把Spark的Driver作为一个Application Master在Yarn集群中先启动，第二阶段由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行任务集，同时监控它的整个运行过程，直到运行完成。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g85mshl8mkj30yy0s2tqu.jpg" alt="image-20191021105131252" style="zoom: 50%;" />

1. 客户端提交应用城促，启动Client向Yarn中提交应用程序，包括启动ApplicationMaster的命令、提交给Application Master的程序和需要需要在Executor中运行的程序等。

2. ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序启动第一个Container运行ApplicationMaster，其中会进行SparkContext的初始化。

3. ApplicationMater向ResourceManager注册，请求资源container。

4. ApplicationMaster请求到资源后，便与NodeManager通信，要求启动CoarseGrainedExecutorBackend，启动后会向ApplicationMaster中的SparkContext注册并申请任务。

5. ApplicationMaster中的SparkContext分配任务集给CoarseGrainedExecutorBackend执行，运行任务会向ApplicationMaster汇报运行的状态和进度。

6. 运用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭。

   

### 5.5 Yarn-Cluster运行模式实现原理



### 5.6 Yarn-Client与Yarn-Cluster对比

​		理解Yarn-Client和Yarn-Cluster区别需要强调下Application Master。在Yarn中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和Resource Manager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。

- Yarn-Client模式下，Application Master仅仅向Yarn请求Executor，Client会和请求的Container通信来调度他们工作，也就说Client不能离开。

  

- Yarn-Cluster模式下，Driver运行在Application Master中，它负责向Yarn申请资源，并监督作业的运行情况。当用户提交了作业之后，就可以关闭Client，作业会继续在Yarn上运行，因而Yarn-Cluster模式不适合运行交互类型的作业。

  



## 6. Mesos运行模式



## 7. 实例演示

